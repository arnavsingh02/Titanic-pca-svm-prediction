{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0a9c0-f0f9-4cec-8db0-aa1443fea14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/arnav/Downloads/titanic.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Handle missing values\n",
    "data['Age'] = data['Age'].fillna(data['Age'].median())\n",
    "data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])\n",
    "\n",
    "# Drop 'Cabin' and 'Ticket' as they are less informative and have many missing values\n",
    "data = data.drop(columns=['Cabin', 'Ticket', 'Name', 'PassengerId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a8503e-6acb-4970-8cd6-fb8abe77a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Encode categorical variables\n",
    "def encode_column(column):\n",
    "    unique_vals = column.unique()\n",
    "    mapping = {val: idx for idx, val in enumerate(unique_vals)}\n",
    "    return column.map(mapping), mapping\n",
    "\n",
    "data['Sex'], sex_mapping = encode_column(data['Sex'])\n",
    "data['Embarked'], embarked_mapping = encode_column(data['Embarked'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['Survived']).values\n",
    "y = data['Survived'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acc2fe-b29f-442c-a96b-c467ab27ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "def train_test_split(X, y, test_size=0.2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    test_size = int(len(X) * test_size)\n",
    "    test_indices, train_indices = indices[:test_size], indices[test_size:]\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcc6d4-0d22-4810-bb29-ec1a225cb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "def standardize_data(X_train, X_test):\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0)\n",
    "    X_train_scaled = (X_train - mean) / std\n",
    "    X_test_scaled = (X_test - mean) / std\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "X_train_scaled, X_test_scaled = standardize_data(X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4a509-a31f-4b8a-b8a9-0b16ca4ca92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Implementation\n",
    "def pca(X, n_components):\n",
    "    # Compute covariance matrix\n",
    "    covariance_matrix = np.cov(X, rowvar=False)\n",
    "    # Eigen decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "    # Sort eigenvalues and eigenvectors\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    # Select top n_components\n",
    "    top_eigenvectors = eigenvectors[:, :n_components]\n",
    "    # Transform data\n",
    "    X_reduced = np.dot(X, top_eigenvectors)\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2b91c-95a6-4e9b-b026-40f72308dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA (keep 95% variance)\n",
    "def explained_variance_ratio(eigenvalues):\n",
    "    total = sum(eigenvalues)\n",
    "    return [val / total for val in eigenvalues]\n",
    "\n",
    "def select_n_components(eigenvalues, threshold=0.95):\n",
    "    ratios = explained_variance_ratio(eigenvalues)\n",
    "    cumulative = np.cumsum(ratios)\n",
    "    n_components = np.argmax(cumulative >= threshold) + 1\n",
    "    return n_components\n",
    "\n",
    "cov_matrix = np.cov(X_train_scaled, rowvar=False)\n",
    "eigenvalues, _ = np.linalg.eigh(cov_matrix)\n",
    "n_components = select_n_components(eigenvalues)\n",
    "X_train_pca = pca(X_train_scaled, n_components)\n",
    "X_test_pca = pca(X_test_scaled, n_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d784e5-c959-4262-b611-959c55f04e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Implementation\n",
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        # Gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)\n",
    "\n",
    "# Train the SVM model on the original features\n",
    "svm = SVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and calculate accuracy for SVM\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "accuracy_svm = np.mean(y_pred_svm == np.where(y_test <= 0, -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a9983-1616-4058-ba22-794a51acb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Implementation\n",
    "class LogisticRegressionCustom:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "\n",
    "            # Gradient descent\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        return [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Logistic Regression without PCA\n",
    "log_reg = LogisticRegressionCustom(learning_rate=0.01, n_iters=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test_scaled)\n",
    "accuracy_log_reg = np.mean(y_pred_log_reg == y_test)\n",
    "\n",
    "# Logistic Regression with PCA\n",
    "log_reg_pca = LogisticRegressionCustom(learning_rate=0.01, n_iters=1000)\n",
    "log_reg_pca.fit(X_train_pca, y_train)\n",
    "y_pred_log_reg_pca = log_reg_pca.predict(X_test_pca)\n",
    "accuracy_log_reg_pca = np.mean(y_pred_log_reg_pca == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c817c3a0-a077-4d13-91bb-6f663fc04157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy (Without PCA): 0.7808988764044944\n",
      "Logistic Regression Accuracy (Without PCA): 0.7921348314606742\n",
      "Logistic Regression Accuracy (With PCA): 0.6123595505617978\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(\"SVM Accuracy (Without PCA):\", accuracy_svm)\n",
    "print(\"Logistic Regression Accuracy (Without PCA):\", accuracy_log_reg)\n",
    "print(\"Logistic Regression Accuracy (With PCA):\", accuracy_log_reg_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c3f91-9da5-440f-85f0-90178d6bc6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
